# Amazon Review Analysis - Pipeline ETL

Pipeline ETL automatis√© pour extraire, transformer et charger les donn√©es d'avis Amazon depuis PostgreSQL vers S3, Snowflake et MongoDB.

> üöÄ **Nouveau sur le projet ?** Consultez [QUICKSTART.md](QUICKSTART.md) pour d√©marrer en 5 minutes !

## D√©marrage Rapide

### 1Ô∏è‚É£ D√©marrage de l'infrastructure Docker

```bash
# 1. Cr√©er le r√©seau Docker (une seule fois)
docker network create review-analysis-network

# 2. PostgreSQL (contient les donn√©es source - initialisation automatique)
docker-compose -f docker-compose.postgres.yml up -d

# 3. MongoDB (stocke les logs et donn√©es rejet√©es)
docker-compose -f docker-compose.mongodb.yml up -d

# 4. Airflow (orchestration du pipeline ETL)
docker-compose -f docker-compose.airflow.yml up -d
```

**‚è±Ô∏è Temps de d√©marrage:**
- PostgreSQL: 1-2 minutes (initialisation automatique des donn√©es au premier lancement)
- Airflow: 1-2 minutes (initialisation de la base de donn√©es Airflow)

### 2Ô∏è‚É£ Configuration des credentials

**Cr√©er le fichier `.env` √† la racine:**
```bash
cp .env.example .env
# √âditer .env avec vos credentials AWS et Snowflake
```

**Variables obligatoires dans `.env`:**
```bash
# AWS S3
AWS_ACCESS_KEY_ID=votre_access_key
AWS_SECRET_ACCESS_KEY=votre_secret_key
AWS_S3_BUCKET=votre-bucket
AWS_REGION=eu-west-1

# Snowflake
SNOWFLAKE_USER=votre_user
SNOWFLAKE_PASSWORD=votre_password
SNOWFLAKE_ACCOUNT=votre_account
SNOWFLAKE_DATABASE=AMAZON_REVIEWS
SNOWFLAKE_SCHEMA=ANALYTICS
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_ROLE=ACCOUNTADMIN

# PostgreSQL et MongoDB sont d√©j√† configur√©s pour Docker
```

### 3Ô∏è‚É£ Configuration Airflow (IMPORTANT!)

**Cr√©er les variables Airflow pour AWS** (√©vite les probl√®mes d'encodage):
```bash
docker exec airflow-webserver airflow variables set AWS_ACCESS_KEY_ID "VOTRE_ACCESS_KEY"
docker exec airflow-webserver airflow variables set AWS_SECRET_ACCESS_KEY "VOTRE_SECRET_KEY"
docker exec airflow-webserver airflow variables set AWS_REGION "eu-west-1"
docker exec airflow-webserver airflow variables set AWS_S3_BUCKET "votre-bucket"
```

### 4Ô∏è‚É£ Lancer le pipeline

**Activer les DAGs (une seule fois):**
```bash
docker exec airflow-webserver airflow dags unpause main_orchestrator
docker exec airflow-webserver airflow dags unpause extract_postgres_to_s3
docker exec airflow-webserver airflow dags unpause transform_load_data
```

**D√©clencher le pipeline complet:**
```bash
docker exec airflow-webserver airflow dags trigger main_orchestrator
```

**Le pipeline ex√©cutera automatiquement:**
1. ‚úÖ Initialisation MongoDB (collections + indexes)
2. ‚úÖ Initialisation Snowflake (database + schema + tables)
3. ‚úÖ Extraction PostgreSQL ‚Üí S3 (8 tables, anonymisation buyer_id)
4. ‚úÖ Transformation et chargement ‚Üí Snowflake + MongoDB

**Monitoring:**
- **Interface Airflow**: http://localhost:8080 (login: admin / password: admin)
- **MongoDB UI**: http://localhost:8081
- **Logs en temps r√©el**: `docker logs -f airflow-scheduler`

---

## Alternative: D√©marrage sans Airflow

### 3. Lancer le pipeline

```bash
cd src_code

# Option A : Pipeline complet (recommand√© pour la premi√®re fois)
python scripts/pipeline.py --all

# Option B : √âtape par √©tape
python scripts/extract_to_s3.py          # PostgreSQL ‚Üí S3
python scripts/process_and_store.py      # S3 ‚Üí Snowflake + MongoDB
```

## Architecture

```
PostgreSQL (Docker)    ‚Üí    AWS S3    ‚Üí    Snowflake
   localhost:5433         (Data Lake)    (Data Warehouse)
   130K+ clients
   42K+ produits                              ‚Üì
   111K+ avis                          MongoDB (Docker)
                                       (Logs & Metadata)
```

## Structure du Projet

```
project_2/
‚îú‚îÄ‚îÄ README.md                              ‚Üê Vous √™tes ici
‚îú‚îÄ‚îÄ .env                                   ‚Üê Configuration centrale (AWS, Snowflake, etc.)
‚îú‚îÄ‚îÄ .env.example                           ‚Üê Template de configuration
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.postgres.yml            ‚Üê PostgreSQL (donn√©es source)
‚îú‚îÄ‚îÄ docker-compose.mongodb.yml             ‚Üê MongoDB (logs & rejets)
‚îú‚îÄ‚îÄ docker-compose.airflow.yml             ‚Üê Airflow (orchestration)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ clean/                             ‚Üê Donn√©es CSV (auto-charg√©es dans PostgreSQL)
‚îú‚îÄ‚îÄ docker/postgres/init/                  ‚Üê Scripts d'initialisation PostgreSQL
‚îÇ
‚îî‚îÄ‚îÄ src_code/
    ‚îú‚îÄ‚îÄ README.md                          ‚Üê Documentation d√©taill√©e du pipeline
    ‚îú‚îÄ‚îÄ scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py                    ‚Üê Pipeline manuel (sans Airflow)
    ‚îÇ   ‚îú‚îÄ‚îÄ extract_to_s3.py               ‚Üê Extraction PostgreSQL ‚Üí S3
    ‚îÇ   ‚îú‚îÄ‚îÄ process_and_store.py           ‚Üê Traitement et stockage
    ‚îÇ   ‚îî‚îÄ‚îÄ dags/                          ‚Üê DAGs Airflow
    ‚îÇ       ‚îú‚îÄ‚îÄ main_orchestrator_dag.py   ‚Üê DAG principal
    ‚îÇ       ‚îú‚îÄ‚îÄ extract_to_s3.py           ‚Üê DAG extraction
    ‚îÇ       ‚îú‚îÄ‚îÄ transform_load_data.py     ‚Üê DAG transformation
    ‚îÇ       ‚îî‚îÄ‚îÄ utils/                     ‚Üê Utilitaires DAGs
    ‚îÇ           ‚îú‚îÄ‚îÄ mongo_handler.py       ‚Üê Gestionnaire MongoDB
    ‚îÇ           ‚îî‚îÄ‚îÄ review_processor.py    ‚Üê Processeur de donn√©es
    ‚îî‚îÄ‚îÄ config/                            ‚Üê Fichiers de configuration
```

## Commandes Utiles

### Gestion des conteneurs

```bash
# Voir les conteneurs en cours
docker ps

# Arr√™ter tous les services
docker-compose -f docker-compose.airflow.yml down
docker-compose -f docker-compose.mongodb.yml down
docker-compose -f docker-compose.postgres.yml down

# Red√©marrer un service sp√©cifique
docker-compose -f docker-compose.airflow.yml restart

# R√©initialiser PostgreSQL (supprime les donn√©es)
docker-compose -f docker-compose.postgres.yml down -v
docker-compose -f docker-compose.postgres.yml up -d
```

### Commandes Airflow

```bash
# Lister les DAGs
docker exec airflow-webserver airflow dags list

# Voir l'√©tat d'un DAG
docker exec airflow-webserver airflow dags list-runs -d main_orchestrator

# Activer/D√©sactiver un DAG
docker exec airflow-webserver airflow dags unpause main_orchestrator
docker exec airflow-webserver airflow dags pause main_orchestrator

# D√©clencher un DAG manuellement
docker exec airflow-webserver airflow dags trigger main_orchestrator

# Voir les t√¢ches d'un DAG run
docker exec airflow-webserver airflow tasks states-for-dag-run transform_load_data <run_id>

# Lister les variables Airflow
docker exec airflow-webserver airflow variables list

# Voir les logs
docker logs -f airflow-scheduler
docker logs -f airflow-webserver
```

### V√©rifier les connexions

```bash
# PostgreSQL
docker exec -it amazon_postgres_db psql -U admin -d amazon_db -c "SELECT COUNT(*) FROM product;"

# MongoDB
docker exec -it amazon-reviews-mongodb mongosh -u admin -p changeme --eval "db.adminCommand('ping')"

# Airflow (v√©rifier qu'il est pr√™t)
docker exec airflow-webserver airflow db check
```

## Donn√©es Disponibles

Le projet contient **~1.7M d'enregistrements** sur 25 tables :
- **130,766** clients
- **42,858** produits
- **222,644** commandes
- **111,322** avis clients
- **100,000** acheteurs
- Et plus...

## Tests de Qualit√©

Le projet inclut une suite compl√®te de tests de qualit√© des donn√©es :

```bash
cd src_code

# Ex√©cuter les tests
python tests/test_data_quality.py

# G√©n√©rer le rapport HTML
python scripts/generate_quality_report.py
```

**8 tests automatis√©s** :
- Connexion PostgreSQL
- Validation des ratings (1-5)
- D√©tection des doublons
- Champs obligatoires non-NULL
- Prix positifs
- Textes non-vides
- Coh√©rence des types
- Int√©grit√© r√©f√©rentielle

Les rapports sont disponibles dans `src_code/reports/` (JSON + HTML).

## Documentation D√©taill√©e

- **`src_code/README.md`** - Documentation compl√®te du pipeline ETL
- **`CONFORMITE_ETL.md`** - Analyse de conformit√© du projet
- **`.env.example`** - Template de configuration avec explications

## Technologies

- **Orchestration** : Apache Airflow 2.8.3 (Docker)
- **Source** : PostgreSQL 17 (Docker)
- **Data Lake** : AWS S3
- **Data Warehouse** : Snowflake
- **Logs & Rejets** : MongoDB 7.0 (Docker)
- **ETL** : Python 3.11+ avec pandas, boto3, snowflake-connector
- **Conteneurisation** : Docker & Docker Compose

## Support

- V√©rifier que Docker est bien install√© et d√©marr√©
- Les ports 5433 (PostgreSQL) et 27017 (MongoDB) doivent √™tre disponibles
- Voir `src_code/README.md` pour le troubleshooting d√©taill√©
