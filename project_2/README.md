# Amazon Review Analysis - Pipeline ETL

Pipeline ETL automatisÃ© pour extraire, transformer et charger les donnÃ©es d'avis Amazon depuis PostgreSQL vers S3, Snowflake et MongoDB.

> ğŸš€ **Nouveau sur le projet ?** Consultez [QUICKSTART.md](QUICKSTART.md) pour dÃ©marrer en 5 minutes !

## DÃ©marrage Rapide

### 1ï¸âƒ£ DÃ©marrage de l'infrastructure Docker

```bash
# 1. CrÃ©er le rÃ©seau Docker (une seule fois)
docker network create review-analysis-network

# 2. PostgreSQL (contient les donnÃ©es source - initialisation automatique)
docker-compose -f docker-compose.postgres.yml up -d

# 3. MongoDB (stocke les logs et donnÃ©es rejetÃ©es)
docker-compose -f docker-compose.mongodb.yml up -d

# 4. Airflow (orchestration du pipeline ETL)
docker-compose -f docker-compose.airflow.yml up -d
```

**â±ï¸ Temps de dÃ©marrage:**
- PostgreSQL: 1-2 minutes (initialisation automatique des donnÃ©es au premier lancement)
- Airflow: 1-2 minutes (initialisation de la base de donnÃ©es Airflow)

### 2ï¸âƒ£ Configuration des credentials

**CrÃ©er le fichier `.env` Ã  la racine:**
```bash
cp .env.example .env
# Ã‰diter .env avec vos credentials AWS et Snowflake
```

**Variables obligatoires dans `.env`:**
```bash
# AWS S3
AWS_ACCESS_KEY_ID=votre_access_key
AWS_SECRET_ACCESS_KEY=votre_secret_key
AWS_S3_BUCKET=votre-bucket
AWS_REGION=eu-west-1

# Snowflake
SNOWFLAKE_USER=votre_user
SNOWFLAKE_PASSWORD=votre_password
SNOWFLAKE_ACCOUNT=votre_account
SNOWFLAKE_DATABASE=AMAZON_REVIEWS
SNOWFLAKE_SCHEMA=ANALYTICS
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_ROLE=ACCOUNTADMIN

# PostgreSQL et MongoDB sont dÃ©jÃ  configurÃ©s pour Docker
```

### 3ï¸âƒ£ Configuration Airflow

Les connexions Airflow (PostgreSQL, MongoDB, Snowflake, AWS S3) sont **automatiquement configurÃ©es** via les variables d'environnement dÃ©finies dans le fichier `.env` (format `AIRFLOW_CONN_*`). Aucune configuration manuelle n'est nÃ©cessaire.

### 4ï¸âƒ£ Lancer le pipeline

**Activer les DAGs (une seule fois):**
```bash
docker exec airflow-webserver airflow dags unpause main_orchestrator
docker exec airflow-webserver airflow dags unpause extract_postgres_to_s3
docker exec airflow-webserver airflow dags unpause transform_load_data
```

**DÃ©clencher le pipeline complet:**
```bash
docker exec airflow-webserver airflow dags trigger main_orchestrator
```

**Le pipeline exÃ©cutera automatiquement:**
1. âœ… Initialisation MongoDB (collections + indexes)
2. âœ… Initialisation Snowflake (database + schema + tables)
3. âœ… Extraction PostgreSQL â†’ S3 (8 tables, anonymisation buyer_id)
4. âœ… Transformation et chargement â†’ Snowflake + MongoDB

**Monitoring:**
- **Interface Airflow**: http://localhost:8080 (login: admin / password: admin)
- **MongoDB UI**: http://localhost:8081
- **Logs en temps rÃ©el**: `docker logs -f airflow-scheduler`

---

## Alternative: DÃ©marrage sans Airflow

### 3. Lancer le pipeline

```bash
cd src_code

# Option A : Pipeline complet (recommandÃ© pour la premiÃ¨re fois)
python scripts/pipeline.py --all

# Option B : Ã‰tape par Ã©tape
python scripts/extract_to_s3.py          # PostgreSQL â†’ S3
python scripts/process_and_store.py      # S3 â†’ Snowflake + MongoDB
```

## Architecture

```
PostgreSQL (Docker)    â†’    AWS S3    â†’    Snowflake
   localhost:5433         (Data Lake)    (Data Warehouse)
   130K+ clients
   42K+ produits                              â†“
   111K+ avis                          MongoDB (Docker)
                                       (Logs & Metadata)
```

## Structure du Projet

```
project_2/
â”œâ”€â”€ README.md                              â† Vous Ãªtes ici
â”œâ”€â”€ .env                                   â† Configuration centrale (AWS, Snowflake, etc.)
â”œâ”€â”€ .env.example                           â† Template de configuration
â”‚
â”œâ”€â”€ docker-compose.postgres.yml            â† PostgreSQL (donnÃ©es source)
â”œâ”€â”€ docker-compose.mongodb.yml             â† MongoDB (logs & rejets)
â”œâ”€â”€ docker-compose.airflow.yml             â† Airflow (orchestration)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ clean/                             â† DonnÃ©es CSV (auto-chargÃ©es dans PostgreSQL)
â”œâ”€â”€ docker/postgres/init/                  â† Scripts d'initialisation PostgreSQL
â”‚
â””â”€â”€ src_code/
    â”œâ”€â”€ README.md                          â† Documentation dÃ©taillÃ©e du pipeline
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ pipeline.py                    â† Pipeline manuel (sans Airflow)
    â”‚   â”œâ”€â”€ extract_to_s3.py               â† Extraction PostgreSQL â†’ S3
    â”‚   â”œâ”€â”€ process_and_store.py           â† Traitement et stockage
    â”‚   â””â”€â”€ dags/                          â† DAGs Airflow
    â”‚       â”œâ”€â”€ main_orchestrator_dag.py   â† DAG principal
    â”‚       â”œâ”€â”€ extract_to_s3.py           â† DAG extraction
    â”‚       â”œâ”€â”€ transform_load_data.py     â† DAG transformation
    â”‚       â””â”€â”€ utils/                     â† Utilitaires DAGs
    â”‚           â”œâ”€â”€ mongo_handler.py       â† Gestionnaire MongoDB
    â”‚           â””â”€â”€ review_processor.py    â† Processeur de donnÃ©es
    â””â”€â”€ config/                            â† Fichiers de configuration
```

## Commandes Utiles

### Gestion des conteneurs

```bash
# Voir les conteneurs en cours
docker ps

# ArrÃªter tous les services
docker-compose -f docker-compose.airflow.yml down
docker-compose -f docker-compose.mongodb.yml down
docker-compose -f docker-compose.postgres.yml down

# RedÃ©marrer un service spÃ©cifique
docker-compose -f docker-compose.airflow.yml restart

# RÃ©initialiser PostgreSQL (supprime les donnÃ©es)
docker-compose -f docker-compose.postgres.yml down -v
docker-compose -f docker-compose.postgres.yml up -d
```

### Commandes Airflow

```bash
# Lister les DAGs
docker exec airflow-webserver airflow dags list

# Voir l'Ã©tat d'un DAG
docker exec airflow-webserver airflow dags list-runs -d main_orchestrator

# Activer/DÃ©sactiver un DAG
docker exec airflow-webserver airflow dags unpause main_orchestrator
docker exec airflow-webserver airflow dags pause main_orchestrator

# DÃ©clencher un DAG manuellement
docker exec airflow-webserver airflow dags trigger main_orchestrator

# Voir les tÃ¢ches d'un DAG run
docker exec airflow-webserver airflow tasks states-for-dag-run transform_load_data <run_id>

# Lister les variables Airflow
docker exec airflow-webserver airflow variables list

# Voir les logs
docker logs -f airflow-scheduler
docker logs -f airflow-webserver
```

### VÃ©rifier les connexions

```bash
# PostgreSQL
docker exec -it amazon_postgres_db psql -U admin -d amazon_db -c "SELECT COUNT(*) FROM product;"

# MongoDB
docker exec -it amazon-reviews-mongodb mongosh -u admin -p changeme --eval "db.adminCommand('ping')"

# Airflow (vÃ©rifier qu'il est prÃªt)
docker exec airflow-webserver airflow db check
```

## DonnÃ©es Disponibles

Le projet contient **~1.7M d'enregistrements** sur 25 tables :
- **130,766** clients
- **42,858** produits
- **222,644** commandes
- **111,322** avis clients
- **100,000** acheteurs
- Et plus...

## Tests de QualitÃ©

Le projet inclut une suite complÃ¨te de tests de qualitÃ© des donnÃ©es :

```bash
cd src_code

# ExÃ©cuter les tests
python tests/test_data_quality.py

# GÃ©nÃ©rer le rapport HTML
python scripts/generate_quality_report.py
```

**8 tests automatisÃ©s** :
- Connexion PostgreSQL
- Validation des ratings (1-5)
- DÃ©tection des doublons
- Champs obligatoires non-NULL
- Prix positifs
- Textes non-vides
- CohÃ©rence des types
- IntÃ©gritÃ© rÃ©fÃ©rentielle

Les rapports sont disponibles dans `src_code/reports/` (JSON + HTML).

## Documentation DÃ©taillÃ©e

- **`src_code/README.md`** - Documentation complÃ¨te du pipeline ETL
- **`CONFORMITE_ETL.md`** - Analyse de conformitÃ© du projet
- **`.env.example`** - Template de configuration avec explications

## Technologies

- **Orchestration** : Apache Airflow 2.8.3 (Docker)
- **Source** : PostgreSQL 17 (Docker)
- **Data Lake** : AWS S3
- **Data Warehouse** : Snowflake
- **Logs & Rejets** : MongoDB 7.0 (Docker)
- **ETL** : Python 3.11+ avec pandas, boto3, snowflake-connector
- **Conteneurisation** : Docker & Docker Compose

## Support

- VÃ©rifier que Docker est bien installÃ© et dÃ©marrÃ©
- Les ports 5433 (PostgreSQL) et 27017 (MongoDB) doivent Ãªtre disponibles
- Voir `src_code/README.md` pour le troubleshooting dÃ©taillÃ©
