# Amazon Review Analysis - Pipeline ETL

Pipeline ETL automatisÃ© pour extraire, transformer et charger les donnÃ©es d'avis Amazon depuis PostgreSQL vers S3, Snowflake et MongoDB.

> ğŸš€ **Nouveau sur le projet ?** Consultez [QUICKSTART.md](QUICKSTART.md) pour dÃ©marrer en 5 minutes !

## DÃ©marrage Rapide (3 Ã©tapes)

### Docker
```bash
# CrÃ©er le rÃ©seau docker
docker network create review-analysis-network

### 1. DÃ©marrer les bases de donnÃ©es

```bash
# PostgreSQL (contient les donnÃ©es source)
docker-compose -f docker-compose.postgres.yml up -d

# MongoDB (stocke les mÃ©tadonnÃ©es du pipeline)
cd src_code
docker-compose -f docker-compose.mongodb.yml up -d

## 1/ DÃ©marrage avec Airflow

# Container Airflow
docker-compose -f docker-compose.airflow.yml up -d

cd ..
```

**Attendre 1-2 minutes** pour que PostgreSQL initialise les donnÃ©es (premiÃ¨re fois uniquement).
**Attendre 1-2 minutes** pour le dÃ©marrage d'Airflow.

### 2. Configurer les credentials

```bash
cd src_code
cp .env.example .env
# Ã‰diter .env avec vos credentials AWS et Snowflake
# PostgreSQL et MongoDB sont dÃ©jÃ  configurÃ©s pour Docker
```
**Configuration Airflow**
**Option 1 : Vous dÃ©finissez vos connexions dans le docker compose Airflow**
**Option 2 : Vous dÃ©finissez vos connexions dans la plateforme Airflow**
## information pour l'option 2 ->
Se rendre dans Admin -> Connections -> Add a new record

**aws_default** Pour Amazon S3
Connection Id : aws_default
Connection Type : Amazon Web Services
AWS Access Key ID : add your key ID
AWS Secret Access Key : add your Secret acces key

**postgres_source** Pour Postgresql Source
Connection Id : postgres_source
Connection Type : Postgres
Host : Container_name
Database : your database
Login : your login
password : your password
Port : 5432 (Port interne)

### DÃ©marrage 
Une fois le container Airflow lancÃ©, les DAGs se dÃ©clencheront seuls.
- Les logs sont enregistrÃ©s dans Mongodb.
- Les rejets sont enregistrÃ©s dans Mongodb.
- Les donnÃ©es cleans sont enregistrÃ©es dans Snowflake.

## 2/ DÃ©marrage sans Airflow

### 3. Lancer le pipeline

```bash
cd src_code

# Option A : Pipeline complet (recommandÃ© pour la premiÃ¨re fois)
python scripts/pipeline.py --all

# Option B : Ã‰tape par Ã©tape
python scripts/extract_to_s3.py          # PostgreSQL â†’ S3
python scripts/process_and_store.py      # S3 â†’ Snowflake + MongoDB
```

## Architecture

```
PostgreSQL (Docker)    â†’    AWS S3    â†’    Snowflake
   localhost:5433         (Data Lake)    (Data Warehouse)
   130K+ clients
   42K+ produits                              â†“
   111K+ avis                          MongoDB (Docker)
                                       (Logs & Metadata)
```

## Structure du Projet

```
project_2/
â”œâ”€â”€ README.md                              â† Vous Ãªtes ici
â”œâ”€â”€ docker-compose.postgres.yml            â† Base de donnÃ©es PostgreSQL
â”œâ”€â”€ .env.local                             â† Config PostgreSQL
â”œâ”€â”€ data/
â”‚   â””â”€â”€ clean/                             â† DonnÃ©es CSV (utilisÃ©es par Docker)
â”œâ”€â”€ docker/postgres/init/                  â† Scripts d'initialisation DB
â””â”€â”€ src_code/                              â† Code du pipeline ETL
    â”œâ”€â”€ README.md                          â† Documentation dÃ©taillÃ©e du pipeline
    â”œâ”€â”€ docker-compose.mongodb.yml         â† Base MongoDB
    â”œâ”€â”€ .env                               â† Configuration du pipeline
    â”œâ”€â”€ scripts/                           â† Scripts Python du pipeline
    â”‚   â”œâ”€â”€ pipeline.py                    â† Script principal
    â”‚   â”œâ”€â”€ extract_to_s3.py               â† Extraction PostgreSQL â†’ S3
    â”‚   â””â”€â”€ process_and_store.py           â† Traitement et stockage
    â””â”€â”€ config/                            â† Fichiers de configuration
```

## Commandes Utiles

### Gestion des conteneurs

```bash
# Voir les conteneurs en cours
docker ps

# ArrÃªter tout
docker-compose -f docker-compose.postgres.yml down
cd src_code && docker-compose -f docker-compose.mongodb.yml down

# RÃ©initialiser PostgreSQL (supprime les donnÃ©es)
docker-compose -f docker-compose.postgres.yml down -v
docker-compose -f docker-compose.postgres.yml up -d
```

### VÃ©rifier les connexions

```bash
# PostgreSQL
docker exec -it amazon_postgres_db psql -U admin -d amazon_db -c "SELECT COUNT(*) FROM product;"

# MongoDB
docker exec -it amazon-reviews-mongodb mongosh -u admin -p changeme --eval "db.adminCommand('ping')"
```

## DonnÃ©es Disponibles

Le projet contient **~1.7M d'enregistrements** sur 25 tables :
- **130,766** clients
- **42,858** produits
- **222,644** commandes
- **111,322** avis clients
- **100,000** acheteurs
- Et plus...

## Tests de QualitÃ©

Le projet inclut une suite complÃ¨te de tests de qualitÃ© des donnÃ©es :

```bash
cd src_code

# ExÃ©cuter les tests
python tests/test_data_quality.py

# GÃ©nÃ©rer le rapport HTML
python scripts/generate_quality_report.py
```

**8 tests automatisÃ©s** :
- Connexion PostgreSQL
- Validation des ratings (1-5)
- DÃ©tection des doublons
- Champs obligatoires non-NULL
- Prix positifs
- Textes non-vides
- CohÃ©rence des types
- IntÃ©gritÃ© rÃ©fÃ©rentielle

Les rapports sont disponibles dans `src_code/reports/` (JSON + HTML).

## Documentation DÃ©taillÃ©e

- **`src_code/README.md`** - Documentation complÃ¨te du pipeline ETL
- **`CONFORMITE_ETL.md`** - Analyse de conformitÃ© du projet
- **`.env.example`** - Template de configuration avec explications

## Technologies

- **Source** : PostgreSQL 17 (Docker)
- **Data Lake** : AWS S3
- **Data Warehouse** : Snowflake
- **Logs** : MongoDB (Docker)
- **ETL** : Python 3.11+

## Support

- VÃ©rifier que Docker est bien installÃ© et dÃ©marrÃ©
- Les ports 5433 (PostgreSQL) et 27017 (MongoDB) doivent Ãªtre disponibles
- Voir `src_code/README.md` pour le troubleshooting dÃ©taillÃ©
