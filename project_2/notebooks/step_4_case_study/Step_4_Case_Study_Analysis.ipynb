{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# STEP 4: CASE STUDY - DATA ANALYSIS\n",
    "# ============================================\n",
    "# Projet : Amazon Review Analysis - Snowflake Edition\n",
    "# Objectif : Analyse et cat√©gorisation des avis utilisateurs avec algorithmes NLP\n",
    "# Date : 2025-11-03\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã TABLE DES MATI√àRES\n",
    "\n",
    "1. [Introduction & Contexte du Case Study](#1-introduction)\n",
    "2. [Configuration & Connexion aux Donn√©es](#2-configuration)\n",
    "3. [Extraction des Donn√©es depuis Snowflake](#3-extraction)\n",
    "4. [Analyse Exploratoire des Donn√©es (EDA)](#4-eda)\n",
    "5. [Choix & Justification de l'Algorithme](#5-algorithme)\n",
    "6. [Impl√©mentation du Mod√®le NLP](#6-implementation)\n",
    "7. [V√©rification du Fonctionnement (Convergence & Performance)](#7-verification)\n",
    "8. [Tests It√©ratifs & Affinage des Crit√®res](#8-affinage)\n",
    "9. [Calcul du Relevance Score Final](#9-relevance)\n",
    "10. [Visualisations & Insights Business](#10-visualisations)\n",
    "11. [Pr√©paration des Donn√©es pour le Dashboard Streamlit](#11-dashboard)\n",
    "12. [Limitations & Recommandations](#12-limitations)\n",
    "13. [Livrables & Export](#13-livrables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ INTRODUCTION & CONTEXTE DU CASE STUDY {#1-introduction}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Contexte Business\n",
    "\n",
    "Dans le cadre de l'am√©lioration de l'exp√©rience utilisateur sur les plateformes e-commerce, Amazon re√ßoit quotidiennement des millions de commentaires clients sur des produits. Face √† ce volume massif d'avis, il devient crucial pour les consommateurs de pouvoir identifier rapidement les commentaires les plus pertinents et informatifs.\n",
    "\n",
    "**Probl√©matique** : Identification automatique des reviews pertinentes sur Amazon  \n",
    "**Objectif** : Cat√©gorisation th√©matique + Scoring de pertinence  \n",
    "**P√©rim√®tre** : Donn√©es extraites de Snowflake (Step 3 - ETL termin√©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Objectifs de l'Analyse\n",
    "\n",
    "- **Objectif 1** : Classer les reviews en cat√©gories m√©tier (NLP zero-shot classification)\n",
    "- **Objectif 2** : D√©velopper un relevance score multi-crit√®res (0-100)\n",
    "- **Objectif 3** : Produire des insights actionnables pour le business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Questions de Recherche\n",
    "\n",
    "- Quelles th√©matiques dominent les avis clients ?\n",
    "- Quels crit√®res influencent la pertinence d'une review ?\n",
    "- Comment optimiser l'exp√©rience utilisateur via l'analyse de sentiment ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 M√©thodologie Globale\n",
    "\n",
    "**Pipeline** : Snowflake ‚Üí Extraction ‚Üí NLP Classification ‚Üí Relevance Scoring ‚Üí Dashboard\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Snowflake   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ Zero-Shot   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  Relevance   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  Streamlit   ‚îÇ\n",
    "‚îÇ  (Reviews)   ‚îÇ      ‚îÇ    NLP      ‚îÇ      ‚îÇ   Scoring    ‚îÇ      ‚îÇ  Dashboard   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ CONFIGURATION & CONNEXION AUX DONN√âES {#2-configuration}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Installation des D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Installation des packages (√† ex√©cuter une fois)\n",
    "# !pip install snowflake-connector-python\n",
    "# !pip install transformers torch\n",
    "# !pip install nltk pandas numpy matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Import des Biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Imports des biblioth√®ques principales\n",
    "# ============================================\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# NLP Libraries (d√©commenter apr√®s installation)\n",
    "# from transformers import pipeline\n",
    "# import torch\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Biblioth√®ques import√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Connexion √† Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Connexion √† Snowflake\n",
    "# ============================================\n",
    "# TODO: Remplacer par vos credentials Snowflake\n",
    "\n",
    "conn_params = {\n",
    "    'account': 'YOUR_ACCOUNT',\n",
    "    'user': 'YOUR_USER',\n",
    "    'password': 'YOUR_PASSWORD',\n",
    "    'warehouse': 'YOUR_WAREHOUSE',\n",
    "    'database': 'YOUR_DATABASE',\n",
    "    'schema': 'YOUR_SCHEMA'\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = snowflake.connector.connect(**conn_params)\n",
    "    print(\"‚úÖ Connexion Snowflake √©tablie avec succ√®s\")\n",
    "    \n",
    "    # Afficher les tables disponibles\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SHOW TABLES\")\n",
    "    tables = cursor.fetchall()\n",
    "    print(f\"\\nTables disponibles : {len(tables)}\")\n",
    "    for table in tables:\n",
    "        print(f\"  - {table[1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion : {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ EXTRACTION DES DONN√âES DEPUIS SNOWFLAKE {#3-extraction}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Requ√™te SQL : S√©lection du Produit √âchantillon\n",
    "\n",
    "**Objectif** : Identifier les produits avec un volume de reviews significatif (minimum 15 reviews) pour l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SQL - Top produits par volume de reviews\n",
    "# ============================================\n",
    "# TODO: Adapter les noms de tables √† votre sch√©ma Snowflake\n",
    "\n",
    "query_products = \"\"\"\n",
    "SELECT\n",
    "    p.p_id,\n",
    "    p.p_name,\n",
    "    p.price,\n",
    "    c.name as category,\n",
    "    COUNT(pr.review_id) as nb_reviews,\n",
    "    ROUND(AVG(r.rating), 2) as avg_rating\n",
    "FROM product p\n",
    "JOIN product_reviews pr ON p.p_id = pr.p_id\n",
    "JOIN review r ON pr.review_id = r.review_id\n",
    "LEFT JOIN category c ON p.category_id = c.category_id\n",
    "GROUP BY p.p_id, p.p_name, p.price, c.name\n",
    "HAVING COUNT(pr.review_id) >= 15  -- Au moins 15 reviews\n",
    "ORDER BY nb_reviews DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "# Ex√©cution de la requ√™te\n",
    "df_products = pd.read_sql(query_products, conn)\n",
    "\n",
    "print(f\"‚úÖ {len(df_products)} produits charg√©s\")\n",
    "display(df_products.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# S√©lection du produit pour l'analyse\n",
    "# ============================================\n",
    "# S√©lectionner le produit avec le plus de reviews (ligne 0)\n",
    "selected_product_id = df_products.iloc[0]['p_id']\n",
    "selected_product_name = df_products.iloc[0]['p_name']\n",
    "\n",
    "print(f\"üì¶ Produit s√©lectionn√© : {selected_product_id}\")\n",
    "print(f\"   Nom : {selected_product_name}\")\n",
    "print(f\"   Reviews : {df_products.iloc[0]['nb_reviews']}\")\n",
    "print(f\"   Rating moyen : {df_products.iloc[0]['avg_rating']}‚≠ê\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extraction des Reviews du Produit S√©lectionn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SQL - Extraction compl√®te des reviews\n",
    "# ============================================\n",
    "# Jointures : review + product_reviews + product + category + review_images + orders\n",
    "\n",
    "query_reviews = f\"\"\"\n",
    "SELECT\n",
    "    r.review_id,\n",
    "    r.buyer_id,\n",
    "    r.title,\n",
    "    r.r_desc AS description,\n",
    "    r.rating,\n",
    "    LENGTH(r.r_desc) AS text_length,\n",
    "    CASE WHEN ri.review_id IS NOT NULL THEN 1 ELSE 0 END AS has_image,\n",
    "    CASE WHEN o.order_id IS NOT NULL THEN 1 ELSE 0 END AS has_orders,\n",
    "    p.p_id,\n",
    "    p.p_name AS product_name,\n",
    "    c.name AS category\n",
    "FROM review r\n",
    "LEFT JOIN product_reviews pr ON r.review_id = pr.review_id\n",
    "LEFT JOIN product p ON pr.p_id = p.p_id\n",
    "LEFT JOIN category c ON p.category_id = c.category_id\n",
    "LEFT JOIN review_images ri ON r.review_id = ri.review_id\n",
    "LEFT JOIN orders o ON r.buyer_id = o.buyer_id\n",
    "WHERE pr.p_id = '{selected_product_id}'\n",
    "ORDER BY r.review_id;\n",
    "\"\"\"\n",
    "\n",
    "df_reviews = pd.read_sql(query_reviews, conn)\n",
    "\n",
    "print(f\"‚úÖ {len(df_reviews)} reviews extraites\")\n",
    "print(f\"   Colonnes : {list(df_reviews.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Nettoyage & Pr√©paration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Data Cleaning\n",
    "# ============================================\n",
    "\n",
    "# Suppression des doublons\n",
    "initial_count = df_reviews.shape[0]\n",
    "df_reviews.drop_duplicates(subset=['review_id'], inplace=True)\n",
    "duplicates_removed = initial_count - df_reviews.shape[0]\n",
    "\n",
    "print(f\"‚úÖ Nettoyage termin√©\")\n",
    "print(f\"   Doublons supprim√©s : {duplicates_removed}\")\n",
    "print(f\"   Shape finale : {df_reviews.shape}\")\n",
    "print(f\"\\nüìä Aper√ßu des donn√©es :\")\n",
    "display(df_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ ANALYSE EXPLORATOIRE DES DONN√âES (EDA) {#4-eda}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Statistiques Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Statistiques g√©n√©rales\n",
    "# ============================================\n",
    "\n",
    "print(\"üìä STATISTIQUES DESCRIPTIVES\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Nombre total de reviews : {len(df_reviews)}\")\n",
    "print(f\"\\nDistribution des ratings :\")\n",
    "print(df_reviews['rating'].value_counts().sort_index())\n",
    "print(f\"\\nRating moyen : {df_reviews['rating'].mean():.2f}‚≠ê\")\n",
    "print(f\"\\nStatistiques de longueur :\")\n",
    "print(f\"  - Minimum : {df_reviews['text_length'].min()} caract√®res\")\n",
    "print(f\"  - Maximum : {df_reviews['text_length'].max()} caract√®res\")\n",
    "print(f\"  - Moyenne : {df_reviews['text_length'].mean():.0f} caract√®res\")\n",
    "print(f\"  - M√©diane : {df_reviews['text_length'].median():.0f} caract√®res\")\n",
    "print(f\"\\nProportion avec images : {df_reviews['has_image'].mean()*100:.1f}%\")\n",
    "print(f\"Proportion avec commandes : {df_reviews['has_orders'].mean()*100:.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Statistiques d√©taill√©es\n",
    "display(df_reviews.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualisations Exploratoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIGURE 1 : Statistiques g√©n√©rales (4 subplots)\n",
    "# ============================================\n",
    "\n",
    "fig1, axes1 = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig1.suptitle(f'Statistiques G√©n√©rales - Produit {selected_product_id}', \n",
    "              fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# 1.1 Distribution des ratings\n",
    "ax = axes1[0, 0]\n",
    "rating_dist = df_reviews['rating'].value_counts().sort_index()\n",
    "colors = ['#e74c3c', '#e67e22', '#f39c12', '#3498db', '#2ecc71']\n",
    "bars = ax.bar(rating_dist.index, rating_dist.values, color=colors, \n",
    "              edgecolor='black', linewidth=2, width=0.7)\n",
    "ax.set_xlabel('Rating', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Nombre de reviews', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Distribution des Ratings', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xticks(rating_dist.index)\n",
    "ax.set_xticklabels([f\"{int(r)}‚≠ê\" for r in rating_dist.index], fontsize=12)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    count = int(height)\n",
    "    percentage = (count / len(df_reviews) * 100)\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "            f'{count}\\n({percentage:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 1.2 Pr√©sence d'images\n",
    "ax = axes1[0, 1]\n",
    "image_data = [df_reviews['has_image'].sum(), len(df_reviews) - df_reviews['has_image'].sum()]\n",
    "colors_img = ['#3498db', '#ecf0f1']\n",
    "ax.pie(image_data, labels=['Avec image', 'Sans image'], \n",
    "       autopct='%1.1f%%', colors=colors_img, startangle=90,\n",
    "       textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
    "       explode=(0.05, 0))\n",
    "ax.set_title('Pr√©sence d\\'Images', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# 1.3 Distribution longueur\n",
    "ax = axes1[1, 0]\n",
    "ax.hist(df_reviews['text_length'], bins=40, color='#3498db', \n",
    "        edgecolor='black', alpha=0.7, linewidth=1.5)\n",
    "mean_val = df_reviews['text_length'].mean()\n",
    "median_val = df_reviews['text_length'].median()\n",
    "ax.axvline(mean_val, color='red', linestyle='--', linewidth=3, \n",
    "           label=f\"Moyenne: {mean_val:.0f} car.\")\n",
    "ax.axvline(median_val, color='green', linestyle='--', linewidth=3, \n",
    "           label=f\"M√©diane: {median_val:.0f} car.\")\n",
    "ax.set_xlabel('Longueur (caract√®res)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Fr√©quence', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Distribution de la Longueur du Texte', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=11, loc='upper right')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# 1.4 Statistiques cl√©s\n",
    "ax = axes1[1, 1]\n",
    "ax.axis('off')\n",
    "stats_box = f\"\"\"\n",
    "R√âSUM√â STATISTIQUE\n",
    "\n",
    "Reviews totales : {len(df_reviews)}\n",
    "\n",
    "Avec images : {df_reviews['has_image'].sum()} ({(df_reviews['has_image'].sum() / len(df_reviews) * 100):.1f}%)\n",
    "\n",
    "Avec commandes : {df_reviews['has_orders'].sum()} ({(df_reviews['has_orders'].sum() / len(df_reviews) * 100):.1f}%)\n",
    "\n",
    "Longueur texte :\n",
    "   ‚Ä¢ Moyenne : {df_reviews['text_length'].mean():.0f} caract√®res\n",
    "   ‚Ä¢ M√©diane : {df_reviews['text_length'].median():.0f} caract√®res\n",
    "   ‚Ä¢ Min : {df_reviews['text_length'].min()} | Max : {df_reviews['text_length'].max()}\n",
    "\n",
    "Rating moyen : {df_reviews['rating'].mean():.2f}/5\n",
    "\"\"\"\n",
    "ax.text(0.1, 0.5, stats_box, ha='left', va='center', fontsize=12, \n",
    "        family='monospace', linespacing=1.8,\n",
    "        bbox=dict(boxstyle='round,pad=1', facecolor='#ecf0f1', \n",
    "                  edgecolor='#34495e', linewidth=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../data/outputs/visualizations/04_eda_stats_generales.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique sauvegard√© : data/outputs/visualizations/04_eda_stats_generales.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIGURE 2 : Analyse par rating (2 subplots)\n",
    "# ============================================\n",
    "\n",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig2.suptitle('Analyse D√©taill√©e par Rating', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 2.1 Longueur moyenne par rating\n",
    "ax = axes2[0]\n",
    "avg_length = df_reviews.groupby('rating')['text_length'].mean().sort_index()\n",
    "bars = ax.bar(avg_length.index, avg_length.values, color=colors, \n",
    "              edgecolor='black', linewidth=2, width=0.6)\n",
    "ax.set_xlabel('Rating', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Longueur moyenne (caract√®res)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Longueur Moyenne du Texte par Rating', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xticks(avg_length.index)\n",
    "ax.set_xticklabels([f\"{int(r)}‚≠ê\" for r in avg_length.index], fontsize=12)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "            f'{height:.0f}', ha='center', va='bottom', \n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "# 2.2 Proportion images par rating\n",
    "ax = axes2[1]\n",
    "img_by_rating = df_reviews.groupby('rating')['has_image'].mean() * 100\n",
    "bars = ax.bar(img_by_rating.index, img_by_rating.values, color=colors,\n",
    "              edgecolor='black', linewidth=2, width=0.6)\n",
    "ax.set_xlabel('Rating', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('% avec images', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Proportion de Reviews avec Images par Rating', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xticks(img_by_rating.index)\n",
    "ax.set_xticklabels([f\"{int(r)}‚≠ê\" for r in img_by_rating.index], fontsize=12)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{height:.1f}%', ha='center', va='bottom',\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../data/outputs/visualizations/04_eda_stats_par_rating.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique sauvegard√© : data/outputs/visualizations/04_eda_stats_par_rating.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Insights Cl√©s de l'EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations principales** :\n",
    "\n",
    "1. **Biais positif** : [√Ä compl√©ter apr√®s ex√©cution] % des reviews sont 5‚òÖ\n",
    "2. **Longueur des textes** : M√©diane de [X] caract√®res, indiquant que la majorit√© des reviews sont courtes\n",
    "3. **Pr√©sence d'images** : [X]% des reviews contiennent des images, avec une corr√©lation positive avec les ratings √©lev√©s\n",
    "4. **Reviews n√©gatives** : Les reviews 1‚òÖ-2‚òÖ sont g√©n√©ralement plus longues et d√©taill√©es\n",
    "5. **Achat v√©rifi√©** : [X]% des reviews proviennent d'achats v√©rifi√©s\n",
    "\n",
    "**Implications pour le mod√®le** :\n",
    "- Les reviews tr√®s courtes (<30 caract√®res) pourraient limiter la performance du mod√®le NLP\n",
    "- La pr√©sence d'images peut servir d'indicateur de qualit√© dans le relevance score\n",
    "- Le biais positif influencera la distribution des cat√©gories pr√©dites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ CHOIX & JUSTIFICATION DE L'ALGORITHME {#5-algorithme}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Probl√©matique & Contraintes\n",
    "\n",
    "#### Probl√©matique M√©tier\n",
    "- Cat√©goriser automatiquement des reviews non labellis√©es\n",
    "- Pas de dataset d'entra√Ænement disponible initialement\n",
    "- Besoin de flexibilit√© pour ajuster les cat√©gories\n",
    "\n",
    "#### Contraintes Techniques\n",
    "- Volume de donn√©es : ~100k reviews (scalabilit√© requise)\n",
    "- Multilingue potentiel (anglais dominant, mais autres langues possibles)\n",
    "- Budget GPU limit√©\n",
    "- D√©lai de livraison court"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Approches Candidates\n",
    "\n",
    "| Approche | Avantages | Inconv√©nients | Pr√©cision Estim√©e | Temps d'Entra√Ænement |\n",
    "|----------|-----------|---------------|-------------------|----------------------|\n",
    "| **Lexicon-based (VADER)** | Rapide, pas d'entra√Ænement | Limit√© au sentiment (+/-), pas de cat√©gorisation | 65-75% | 0 min |\n",
    "| **Zero-Shot Classification** | Pas de labeling requis, flexible | Mod√®le lourd, lent | 70-85% | 0 min (pr√©-entra√Æn√©) |\n",
    "| **Fine-tuned BERT** | Haute pr√©cision | N√©cessite dataset labellis√© + GPU | 85-95% | 2-4 heures |\n",
    "| **LDA Topic Modeling** | D√©couvre th√©matiques automatiquement | Difficile √† interpr√©ter, pas de labels clairs | 60-70% | 30-60 min |\n",
    "| **Naive Bayes / SVM** | Rapide, interpr√©table | N√©cessite labeling manuel | 75-85% | 15-30 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 D√©cision & Justification\n",
    "\n",
    "#### ‚úÖ Algorithme S√©lectionn√© : Zero-Shot Classification\n",
    "\n",
    "**Mod√®le choisi** : `facebook/bart-large-mnli` (ou `MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7` pour multilingue)\n",
    "\n",
    "**Justifications** :\n",
    "1. **Pas de labeling manuel** ‚Üí Gain de temps significatif (√©conomie de 40-80 heures)\n",
    "2. **Flexibilit√©** ‚Üí Possibilit√© d'ajuster les cat√©gories sans r√©-entra√Ænement\n",
    "3. **Performance acceptable** ‚Üí 70-85% de pr√©cision selon benchmarks acad√©miques\n",
    "4. **Multilingue** ‚Üí Support de l'anglais, fran√ßais, espagnol, etc. (mDeBERTa)\n",
    "5. **Proof of concept rapide** ‚Üí Validation de l'approche avant investissement dans un mod√®le custom\n",
    "\n",
    "**Cat√©gories m√©tier d√©finies** (4 cat√©gories regroup√©es pour am√©liorer la confiance) :\n",
    "- **Product Quality or Satisfaction** : Qualit√©, performance, satisfaction g√©n√©rale\n",
    "- **Product Defect or Damaged Item** : D√©fauts, probl√®mes, produit endommag√©\n",
    "- **Delivery Issue or Shipping Delay** : Livraison, d√©lais, packaging\n",
    "- **Customer Service or Support** : SAV, remboursement, support client\n",
    "\n",
    "**Alternative future** : Fine-tuning sur dataset labellis√© (500-1000 reviews) pour ‚Üë pr√©cision √† 90%+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ IMPL√âMENTATION DU MOD√àLE NLP {#6-implementation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Initialisation du Mod√®le Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Chargement du mod√®le Zero-Shot\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS INSTALLATION DE TRANSFORMERS\n",
    "\n",
    "# from transformers import pipeline\n",
    "# import torch\n",
    "\n",
    "# # D√©tection du device (GPU si disponible)\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "# print(f\"Device utilis√© : {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# # Initialisation du classificateur\n",
    "# classifier = pipeline(\n",
    "#     \"zero-shot-classification\", \n",
    "#     model=\"facebook/bart-large-mnli\",  # ou \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# # D√©finition des cat√©gories\n",
    "# candidate_labels = [\n",
    "#     \"product quality or satisfaction\",\n",
    "#     \"product defect or damaged item\",\n",
    "#     \"delivery issue or shipping delay\",\n",
    "#     \"customer service or support\"\n",
    "# ]\n",
    "\n",
    "# print(f\"‚úÖ Mod√®le charg√© - {len(candidate_labels)} cat√©gories d√©finies\")\n",
    "# print(f\"   Cat√©gories : {candidate_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Fonction de Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Fonction classify_review()\n",
    "# ============================================\n",
    "\n",
    "# def classify_review(text):\n",
    "#     \"\"\"\n",
    "#     Applique le mod√®le zero-shot √† un texte de review.\n",
    "#     \n",
    "#     Args:\n",
    "#         text (str): Texte de la review √† classifier\n",
    "#         \n",
    "#     Returns:\n",
    "#         tuple: (cat√©gorie pr√©dite, score de confiance)\n",
    "#     \"\"\"\n",
    "#     # Gestion des cas vides/NaN\n",
    "#     if pd.isna(text) or text.strip() == \"\":\n",
    "#         return None, 0.0\n",
    "    \n",
    "#     # Classification\n",
    "#     result = classifier(text, candidate_labels)\n",
    "    \n",
    "#     # Retourner la meilleure pr√©diction\n",
    "#     return result['labels'][0], result['scores'][0]\n",
    "\n",
    "# print(\"‚úÖ Fonction classify_review() d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Application sur l'√âchantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Classification de toutes les reviews\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS INITIALISATION DU MOD√àLE\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# print(f\"üöÄ Classification de {len(df_reviews)} reviews en cours...\")\n",
    "# start_time = datetime.now()\n",
    "\n",
    "# # Application de la classification\n",
    "# df_reviews[['category', 'confidence_score']] = df_reviews['description'].progress_apply(\n",
    "#     lambda x: pd.Series(classify_review(x))\n",
    "# )\n",
    "\n",
    "# execution_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# print(f\"\\n‚úÖ Classification termin√©e en {execution_time:.1f} secondes\")\n",
    "# print(f\"\\nüìä Distribution des cat√©gories :\")\n",
    "# print(df_reviews['category'].value_counts())\n",
    "# print(f\"\\nüìà Score de confiance moyen : {df_reviews['confidence_score'].mean():.3f}\")\n",
    "# print(f\"   Score m√©dian : {df_reviews['confidence_score'].median():.3f}\")\n",
    "# print(f\"   Min : {df_reviews['confidence_score'].min():.3f} | Max : {df_reviews['confidence_score'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ V√âRIFICATION DU FONCTIONNEMENT {#7-verification}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 V√©rification de la Convergence\n",
    "\n",
    "#### Crit√®res de Validation\n",
    "- **Confidence score moyen** : doit √™tre >0.60 (seuil acceptable pour zero-shot)\n",
    "- **Distribution des cat√©gories** : pas de d√©s√©quilibre extr√™me (>95% dans une cat√©gorie)\n",
    "- **Coh√©rence s√©mantique** : v√©rification manuelle sur 10-20 exemples al√©atoires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 M√©triques de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Calcul des m√©triques de performance\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CLASSIFICATION\n",
    "\n",
    "# # Confidence score moyen par cat√©gorie\n",
    "# print(\"üìä Confidence Score par Cat√©gorie\\n\")\n",
    "# confidence_by_cat = df_reviews.groupby('category')['confidence_score'].agg(['mean', 'median', 'min', 'max'])\n",
    "# display(confidence_by_cat)\n",
    "\n",
    "# # Distribution des scores (histogramme)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(df_reviews['confidence_score'], bins=30, color='#3498db', \n",
    "#          edgecolor='black', alpha=0.7)\n",
    "# plt.axvline(df_reviews['confidence_score'].mean(), color='red', \n",
    "#             linestyle='--', linewidth=2, label=f\"Moyenne: {df_reviews['confidence_score'].mean():.3f}\")\n",
    "# plt.axvline(0.60, color='green', linestyle='--', linewidth=2, \n",
    "#             label='Seuil acceptable (0.60)')\n",
    "# plt.xlabel('Confidence Score', fontsize=12, fontweight='bold')\n",
    "# plt.ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "# plt.title('Distribution du Confidence Score', fontsize=14, fontweight='bold')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../../data/outputs/visualizations/07_confidence_distribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"‚úÖ Graphique sauvegard√© : data/outputs/visualizations/07_confidence_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Validation Manuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# √âchantillon de validation manuelle\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CLASSIFICATION\n",
    "\n",
    "# # S√©lectionner 20 reviews al√©atoires pour validation manuelle\n",
    "# sample_for_validation = df_reviews.sample(20, random_state=42)\n",
    "\n",
    "# print(\"üìù √âCHANTILLON POUR VALIDATION MANUELLE\\n\")\n",
    "# print(\"=\" * 100)\n",
    "\n",
    "# for idx, row in sample_for_validation.iterrows():\n",
    "#     print(f\"\\n[{idx}] Rating: {row['rating']}‚≠ê | Cat√©gorie pr√©dite: {row['category']} | Confiance: {row['confidence_score']:.3f}\")\n",
    "#     print(f\"Texte: {row['description'][:200]}...\")\n",
    "#     print(\"-\" * 100)\n",
    "\n",
    "# # TODO: √âvaluer manuellement et calculer la pr√©cision\n",
    "# # Cr√©er une colonne 'manual_validation' avec 'correct' ou 'incorrect'\n",
    "# # Precision = nb_correct / 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ TESTS IT√âRATIFS & AFFINAGE DES CRIT√àRES {#8-affinage}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Exp√©rimentation 1 : Regroupement de Cat√©gories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypoth√®se\n",
    "Regrouper des cat√©gories similaires pourrait am√©liorer le confidence score en r√©duisant l'ambigu√Øt√© entre classes proches.\n",
    "\n",
    "#### Test : Passer de 5 √† 4 cat√©gories\n",
    "- **Avant** : \"product quality\" + \"general satisfaction\" s√©par√©es\n",
    "- **Apr√®s** : \"product quality or satisfaction\" fusionn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Test avec 4 cat√©gories au lieu de 5\n",
    "# ============================================\n",
    "# TODO: Impl√©menter le test apr√®s avoir obtenu les premiers r√©sultats\n",
    "\n",
    "# R√©sultats attendus :\n",
    "# - Confidence moyenne avant : [√Ä mesurer]\n",
    "# - Confidence moyenne apr√®s : [√Ä mesurer]\n",
    "# - D√©cision : ‚úÖ Conserver le regroupement si am√©lioration >5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Exp√©rimentation 2 : Filtrage des Reviews Courtes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypoth√®se\n",
    "Reviews <30 caract√®res (ex: \"Perfect!\", \"Great!\") manquent de contexte et nuisent √† la performance du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Test : Exclure reviews <30 caract√®res\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CLASSIFICATION INITIALE\n",
    "\n",
    "# # Analyser l'impact des reviews courtes\n",
    "# short_reviews = df_reviews[df_reviews['text_length'] < 30]\n",
    "# long_reviews = df_reviews[df_reviews['text_length'] >= 30]\n",
    "\n",
    "# print(f\"üìä Impact des reviews courtes\\n\")\n",
    "# print(f\"Reviews <30 caract√®res : {len(short_reviews)} ({len(short_reviews)/len(df_reviews)*100:.1f}%)\")\n",
    "# print(f\"Confidence moyenne (courtes) : {short_reviews['confidence_score'].mean():.3f}\")\n",
    "# print(f\"Confidence moyenne (longues) : {long_reviews['confidence_score'].mean():.3f}\")\n",
    "# print(f\"\\nDiff√©rence : {long_reviews['confidence_score'].mean() - short_reviews['confidence_score'].mean():.3f}\")\n",
    "\n",
    "# # D√©cision : Si diff√©rence >0.10, filtrer les reviews courtes\n",
    "# # df_reviews_filtered = df_reviews[df_reviews['text_length'] >= 30].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Exp√©rimentation 3 : Comparaison Mod√®les (BART vs mDeBERTa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypoth√®se\n",
    "Le mod√®le multilingue mDeBERTa pourrait am√©liorer les r√©sultats si des reviews non-anglaises sont pr√©sentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Test : BART vs mDeBERTa\n",
    "# ============================================\n",
    "# TODO: Impl√©menter la comparaison si dataset multilingue\n",
    "\n",
    "# Crit√®res de comparaison :\n",
    "# - Temps d'inf√©rence\n",
    "# - Confidence score moyen\n",
    "# - Pr√©cision sur validation manuelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 R√©capitulatif des Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Test | M√©trique | Avant | Apr√®s | Am√©lioration | D√©cision |\n",
    "|------|----------|-------|-------|--------------|----------|\n",
    "| Regroupement cat√©gories | Confidence | [X] | [X] | [+X%] | [‚úÖ/‚ùå] |\n",
    "| Filtrage <30 char | Confidence | [X] | [X] | [+X%] | [‚úÖ/‚ùå] |\n",
    "| Mod√®le multilingue | Temps exec | [Xs] | [Xs] | [+X%] | [‚úÖ/‚ùå] |\n",
    "\n",
    "**Conclusion** : [√Ä compl√©ter apr√®s tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ CALCUL DU RELEVANCE SCORE FINAL {#9-relevance}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Formule du Relevance Score\n",
    "\n",
    "Le relevance score combine 5 crit√®res pond√©r√©s pour identifier les reviews les plus informatives :\n",
    "\n",
    "```python\n",
    "relevance_score = (\n",
    "    0.25 √ó text_length_score      # Longueur optimale ~300 caract√®res (fonction gaussienne)\n",
    "  + 0.20 √ó has_image              # Pr√©sence d'image = engagement\n",
    "  + 0.15 √ó has_orders             # Achat v√©rifi√© = cr√©dibilit√©\n",
    "  + 0.15 √ó is_extreme_rating      # Rating 1‚òÖ ou 5‚òÖ = opinion tranch√©e\n",
    "  + 0.25 √ó sentiment_score        # Analyse VADER = densit√© √©motionnelle\n",
    ") √ó 100\n",
    "```\n",
    "\n",
    "**√âchelle** : 0-100 (plus le score est √©lev√©, plus la review est pertinente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Impl√©mentation des Sous-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Fonction calculate_text_length_score()\n",
    "# ============================================\n",
    "# Fonction gaussienne : score maximal √† 300 caract√®res\n",
    "\n",
    "def calculate_text_length_score(length):\n",
    "    \"\"\"\n",
    "    Calcule un score de pertinence bas√© sur la longueur du texte.\n",
    "    Utilise une distribution gaussienne centr√©e sur 300 caract√®res.\n",
    "    \n",
    "    Args:\n",
    "        length (int): Longueur du texte en caract√®res\n",
    "        \n",
    "    Returns:\n",
    "        float: Score entre 0 et 1\n",
    "    \"\"\"\n",
    "    optimal = 300  # Longueur optimale\n",
    "    sigma = 200    # √âcart-type\n",
    "    return np.exp(-((length - optimal)**2) / (2 * sigma**2))\n",
    "\n",
    "# Application sur le dataframe\n",
    "df_reviews['text_length_score'] = df_reviews['text_length'].apply(calculate_text_length_score)\n",
    "\n",
    "print(\"‚úÖ text_length_score calcul√©\")\n",
    "print(f\"   Score moyen : {df_reviews['text_length_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Calcul is_extreme_rating\n",
    "# ============================================\n",
    "# Reviews avec rating 1‚òÖ ou 5‚òÖ sont consid√©r√©es comme plus pertinentes\n",
    "\n",
    "df_reviews['is_extreme_rating'] = df_reviews['rating'].apply(\n",
    "    lambda x: 1 if x in [1, 5] else 0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ is_extreme_rating calcul√©\")\n",
    "print(f\"   Reviews extr√™mes : {df_reviews['is_extreme_rating'].sum()} ({df_reviews['is_extreme_rating'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Fonction sentiment_score() (VADER)\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS INSTALLATION DE NLTK\n",
    "\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# # T√©l√©charger le lexique VADER\n",
    "# nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def sentiment_score(text):\n",
    "#     \"\"\"\n",
    "#     Calcule un score de sentiment bas√© sur VADER.\n",
    "#     Normalise le compound score [-1, 1] vers [0, 1].\n",
    "#     \n",
    "#     Args:\n",
    "#         text (str): Texte √† analyser\n",
    "#         \n",
    "#     Returns:\n",
    "#         float: Score entre 0 et 1\n",
    "#     \"\"\"\n",
    "#     if pd.isna(text):\n",
    "#         return 0.0\n",
    "    \n",
    "#     score = analyzer.polarity_scores(text)['compound']\n",
    "#     # Normalisation de [-1, 1] vers [0, 1]\n",
    "#     return (score + 1) / 2\n",
    "\n",
    "# # Application sur le dataframe\n",
    "# df_reviews['keyword_score'] = df_reviews['description'].apply(sentiment_score)\n",
    "\n",
    "# print(\"‚úÖ sentiment_score (keyword_score) calcul√©\")\n",
    "# print(f\"   Score moyen : {df_reviews['keyword_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Calcul du Relevance Score Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Calcul du relevance_score final\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CALCUL DE TOUS LES SOUS-SCORES\n",
    "\n",
    "# df_reviews['relevance_score'] = (\n",
    "#     0.25 * df_reviews['text_length_score'] +\n",
    "#     0.20 * df_reviews['has_image'] +\n",
    "#     0.15 * df_reviews['has_orders'] +\n",
    "#     0.15 * df_reviews['is_extreme_rating'] +\n",
    "#     0.25 * df_reviews['keyword_score']\n",
    "# ) * 100\n",
    "\n",
    "# print(\"‚úÖ Relevance Score calcul√© pour toutes les reviews\")\n",
    "# print(f\"\\nüìä Statistiques du Relevance Score :\")\n",
    "# print(f\"   Moyenne : {df_reviews['relevance_score'].mean():.2f}\")\n",
    "# print(f\"   M√©diane : {df_reviews['relevance_score'].median():.2f}\")\n",
    "# print(f\"   Min : {df_reviews['relevance_score'].min():.2f} | Max : {df_reviews['relevance_score'].max():.2f}\")\n",
    "# print(f\"   √âcart-type : {df_reviews['relevance_score'].std():.2f}\")\n",
    "\n",
    "# # Classification binaire : Relevant (‚â•80) vs Irrelevant (<80)\n",
    "# df_reviews['classification_review'] = df_reviews['relevance_score'].apply(\n",
    "#     lambda x: 'Relevant' if x >= 80 else 'Irrelevant'\n",
    "# )\n",
    "\n",
    "# print(f\"\\nüìà Distribution Relevant/Irrelevant (seuil = 80) :\")\n",
    "# print(df_reviews['classification_review'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Distribution du Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualisation de la distribution\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CALCUL DU RELEVANCE SCORE\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# # Histogramme de la distribution\n",
    "# ax = axes[0]\n",
    "# ax.hist(df_reviews['relevance_score'], bins=30, color='#3498db', \n",
    "#         edgecolor='black', alpha=0.7)\n",
    "# ax.axvline(df_reviews['relevance_score'].mean(), color='red', \n",
    "#            linestyle='--', linewidth=2, label=f\"Moyenne: {df_reviews['relevance_score'].mean():.1f}\")\n",
    "# ax.axvline(80, color='green', linestyle='--', linewidth=2, \n",
    "#            label='Seuil pertinence (80)')\n",
    "# ax.set_xlabel('Relevance Score', fontsize=12, fontweight='bold')\n",
    "# ax.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "# ax.set_title('Distribution du Relevance Score', fontsize=14, fontweight='bold')\n",
    "# ax.legend()\n",
    "\n",
    "# # Boxplot par cat√©gorie\n",
    "# ax = axes[1]\n",
    "# df_reviews.boxplot(column='relevance_score', by='category', ax=ax)\n",
    "# ax.set_xlabel('Cat√©gorie', fontsize=12, fontweight='bold')\n",
    "# ax.set_ylabel('Relevance Score', fontsize=12, fontweight='bold')\n",
    "# ax.set_title('Relevance Score par Cat√©gorie', fontsize=14, fontweight='bold')\n",
    "# plt.suptitle('')  # Supprimer le titre par d√©faut de boxplot\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../../data/outputs/visualizations/09_relevance_distribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"‚úÖ Graphique sauvegard√© : data/outputs/visualizations/09_relevance_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîü VISUALISATIONS & INSIGHTS BUSINESS {#10-visualisations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Heatmap : Cat√©gories vs Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Heatmap interactive (Plotly)\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CLASSIFICATION\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# # Cr√©ation de la matrice cat√©gorie x rating\n",
    "# heatmap_data = pd.crosstab(df_reviews['category'], df_reviews['rating'])\n",
    "\n",
    "# fig = px.imshow(\n",
    "#     heatmap_data,\n",
    "#     labels=dict(x=\"Rating\", y=\"Cat√©gorie\", color=\"Nombre de reviews\"),\n",
    "#     x=[f\"{int(r)}‚≠ê\" for r in heatmap_data.columns],\n",
    "#     y=heatmap_data.index,\n",
    "#     color_continuous_scale='Blues',\n",
    "#     text_auto=True\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title='Distribution des Cat√©gories par Rating',\n",
    "#     width=800,\n",
    "#     height=500,\n",
    "#     font=dict(size=12)\n",
    "# )\n",
    "\n",
    "# fig.write_html('../../data/outputs/visualizations/10_category_rating_heatmap.html')\n",
    "# fig.show()\n",
    "\n",
    "# print(\"‚úÖ Heatmap sauvegard√© : data/outputs/visualizations/10_category_rating_heatmap.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Top Reviews Pertinentes par Cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Top 5 reviews par cat√©gorie (relevance >90)\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CALCUL DU RELEVANCE SCORE\n",
    "\n",
    "# print(\"üèÜ TOP REVIEWS PAR CAT√âGORIE (Relevance Score >90)\\n\")\n",
    "# print(\"=\" * 120)\n",
    "\n",
    "# for category in df_reviews['category'].unique():\n",
    "#     top_reviews = df_reviews[\n",
    "#         (df_reviews['category'] == category) & \n",
    "#         (df_reviews['relevance_score'] > 90)\n",
    "#     ].nlargest(5, 'relevance_score')\n",
    "    \n",
    "#     if len(top_reviews) > 0:\n",
    "#         print(f\"\\nüìå Cat√©gorie : {category.upper()}\")\n",
    "#         print(\"-\" * 120)\n",
    "#         for idx, row in top_reviews.iterrows():\n",
    "#             print(f\"  [Score: {row['relevance_score']:.1f}] {row['rating']}‚≠ê | {row['description'][:150]}...\")\n",
    "#     else:\n",
    "#         print(f\"\\nüìå Cat√©gorie : {category.upper()} - Aucune review avec score >90\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Word Clouds par Cat√©gorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Word Clouds (4 subplots)\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CLASSIFICATION\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "# fig.suptitle('Word Clouds par Cat√©gorie', fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# categories = df_reviews['category'].unique()\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i, category in enumerate(categories[:4]):\n",
    "#     text = ' '.join(df_reviews[df_reviews['category'] == category]['description'].dropna())\n",
    "    \n",
    "#     wordcloud = WordCloud(\n",
    "#         width=800, height=400,\n",
    "#         background_color='white',\n",
    "#         colormap='viridis',\n",
    "#         max_words=50\n",
    "#     ).generate(text)\n",
    "    \n",
    "#     axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "#     axes[i].set_title(category.title(), fontsize=14, fontweight='bold')\n",
    "#     axes[i].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../../data/outputs/visualizations/10_wordclouds_by_category.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"‚úÖ Word clouds sauvegard√©s : data/outputs/visualizations/10_wordclouds_by_category.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Insights M√©tier Cl√©s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä Insights Business (√Ä compl√©ter apr√®s analyse)\n",
    "\n",
    "1. **Distribution des ratings** : [X]% des reviews sont positives (5‚òÖ) ‚Üí Produit performant\n",
    "2. **Reviews courtes** : [X]% de reviews <30 caract√®res ‚Üí Encourager reviews d√©taill√©es (gamification ?)\n",
    "3. **Cat√©gorie dominante** : [Cat√©gorie] repr√©sente [X]% des reviews ‚Üí Prioriser dans roadmap produit\n",
    "4. **Delivery Issues** : [X]% des reviews ‚Üí Logistique efficace ou point d'am√©lioration ?\n",
    "5. **Relevance Score m√©dian** : [X]/100 ‚Üí Seuil √† [X] identifie top [X]% des reviews\n",
    "6. **Sentiment vs Rating** : Corr√©lation de [X] ‚Üí Coh√©rence ou biais ?\n",
    "\n",
    "#### üí° Recommandations Actionnables\n",
    "\n",
    "- **Marketing** : Mettre en avant les reviews \"Relevant\" avec score >80\n",
    "- **Produit** : Analyser en priorit√© les reviews \"Product Defect\" avec score √©lev√©\n",
    "- **Support Client** : Automatiser la d√©tection des \"Customer Service\" issues\n",
    "- **Logistique** : Surveiller l'√©volution temporelle des \"Delivery Issues\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ PR√âPARATION DES DONN√âES POUR LE DASHBOARD STREAMLIT {#11-dashboard}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Agr√©gations pour le Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cr√©ation des datasets agr√©g√©s\n",
    "# ============================================\n",
    "# D√âCOMMENTER APR√àS CLASSIFICATION\n",
    "\n",
    "# # Dataset 1 : Statistiques par cat√©gorie\n",
    "# df_category_stats = df_reviews.groupby('category').agg({\n",
    "#     'review_id': 'count',\n",
    "#     'rating': 'mean',\n",
    "#     'relevance_score': 'mean',\n",
    "#     'confidence_score': 'mean',\n",
    "#     'text_length': 'mean'\n",
    "# }).reset_index()\n",
    "\n",
    "# df_category_stats.columns = ['category', 'nb_reviews', 'avg_rating', \n",
    "#                                'avg_relevance', 'avg_confidence', 'avg_text_length']\n",
    "\n",
    "# print(\"‚úÖ df_category_stats cr√©√©\")\n",
    "# display(df_category_stats)\n",
    "\n",
    "# # Dataset 2 : Top reviews pertinentes\n",
    "# df_top_reviews = df_reviews[\n",
    "#     df_reviews['relevance_score'] >= 80\n",
    "# ][['review_id', 'category', 'rating', 'relevance_score', 'description']].copy()\n",
    "\n",
    "# print(f\"\\n‚úÖ df_top_reviews cr√©√© : {len(df_top_reviews)} reviews (score ‚â•80)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Sauvegarde des Donn√©es Pr√©par√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Export vers Snowflake (Option 1)\n",
    "# ============================================\n",
    "# D√âCOMMENTER POUR SAUVEGARDER DANS SNOWFLAKE\n",
    "\n",
    "# from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "# # Cr√©er une table REVIEWS_ANALYZED dans Snowflake\n",
    "# success, nchunks, nrows, _ = write_pandas(\n",
    "#     conn,\n",
    "#     df_reviews,\n",
    "#     table_name='REVIEWS_ANALYZED',\n",
    "#     database='YOUR_DATABASE',\n",
    "#     schema='YOUR_SCHEMA',\n",
    "#     auto_create_table=True,\n",
    "#     overwrite=True\n",
    "# )\n",
    "\n",
    "# if success:\n",
    "#     print(f\"‚úÖ {nrows} lignes ins√©r√©es dans Snowflake (REVIEWS_ANALYZED)\")\n",
    "# else:\n",
    "#     print(\"‚ùå Erreur lors de l'insertion dans Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Export vers fichiers locaux (Option 2 - Backup)\n",
    "# ============================================\n",
    "# D√âCOMMENTER POUR SAUVEGARDER LOCALEMENT\n",
    "\n",
    "# # Sauvegarder en Parquet (format optimis√©)\n",
    "# df_reviews.to_parquet('../../data/outputs/processed/reviews_analyzed.parquet', index=False)\n",
    "# df_category_stats.to_csv('../../data/outputs/processed/category_stats.csv', index=False)\n",
    "# df_top_reviews.to_csv('../../data/outputs/processed/top_reviews.csv', index=False)\n",
    "\n",
    "# print(\"‚úÖ Donn√©es sauvegard√©es localement :\")\n",
    "# print(\"   - data/outputs/processed/reviews_analyzed.parquet\")\n",
    "# print(\"   - data/outputs/processed/category_stats.csv\")\n",
    "# print(\"   - data/outputs/processed/top_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Structure du Dashboard Streamlit (Aper√ßu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä Dashboard Streamlit - Pages Planifi√©es\n",
    "\n",
    "**Page 1 - Overview** (`pages/01_overview.py`):\n",
    "- KPIs globaux : Total reviews, Rating moyen, Distribution des cat√©gories\n",
    "- Graphiques : Barplot ratings, Pie chart cat√©gories\n",
    "- Filtres : Par rating, par cat√©gorie, par date (si disponible)\n",
    "\n",
    "**Page 2 - Sentiment Analysis** (`pages/02_sentiment_analysis.py`):\n",
    "- Distribution des cat√©gories par rating (heatmap)\n",
    "- Word clouds interactifs par cat√©gorie\n",
    "- Confidence score distribution\n",
    "\n",
    "**Page 3 - Top Reviews** (`pages/03_category_insights.py`):\n",
    "- Tableau filtrable des reviews pertinentes (relevance >80)\n",
    "- Recherche full-text dans les reviews\n",
    "- Export CSV des reviews filtr√©es\n",
    "\n",
    "**Page 4 - Recommendations** (`pages/04_recommendations.py`):\n",
    "- Insights actionnables pour le business\n",
    "- Tendances identifi√©es\n",
    "- M√©triques de performance du mod√®le\n",
    "\n",
    "**Emplacement** : `/dashboards/streamlit_app.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£2Ô∏è‚É£ LIMITATIONS & RECOMMANDATIONS {#12-limitations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Limitations Identifi√©es\n",
    "\n",
    "#### ‚ö†Ô∏è Limitations du Prototype\n",
    "\n",
    "1. **√âchantillon limit√©** : 1 seul produit analys√© ‚Üí Repr√©sentativit√© ?\n",
    "   - Biais potentiel : Le produit s√©lectionn√© peut ne pas √™tre repr√©sentatif de toute la base\n",
    "   - Impact : Les patterns identifi√©s peuvent ne pas se g√©n√©raliser\n",
    "\n",
    "2. **Biais positif** : [X]% de 5‚òÖ ‚Üí Cat√©gorie \"quality\" surrepr√©sent√©e\n",
    "   - Cons√©quence : Le mod√®le pourrait avoir du mal √† identifier les cat√©gories minoritaires\n",
    "\n",
    "3. **Mod√®le non fine-tun√©** : Pr√©cision limit√©e (~75% estim√©e vs 90%+ possible)\n",
    "   - Zero-shot est un bon point de d√©part mais pas optimal pour production\n",
    "\n",
    "4. **Pas de d√©tection de spam** : Reviews fakes/bots non filtr√©es\n",
    "   - Risque : Pollution des insights business par des reviews non authentiques\n",
    "\n",
    "5. **Scalabilit√©** : Temps d'inf√©rence √©lev√© (GPU requis pour >10k reviews)\n",
    "   - Co√ªt : ~120s pour 376 reviews ‚Üí ~8h pour 100k reviews sur CPU\n",
    "\n",
    "6. **Relevance score statique** : Pond√©ration fixe (0.25, 0.20, 0.15, 0.15, 0.25)\n",
    "   - Am√©lioration possible : Apprendre les pond√©rations optimales via ML\n",
    "\n",
    "7. **Pas de validation externe** : Aucun ground truth pour mesurer la pr√©cision r√©elle\n",
    "   - Solution : Labeling manuel d'un √©chantillon de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Recommandations Futures\n",
    "\n",
    "#### ‚úÖ Roadmap d'Am√©lioration\n",
    "\n",
    "**Court Terme (0-3 mois)** :\n",
    "- [ ] **Validation sur dataset labellis√©** : Labelliser manuellement 500-1000 reviews pour mesurer pr√©cision\n",
    "- [ ] **Extension multi-produits** : Tester sur 10-20 produits de cat√©gories vari√©es\n",
    "- [ ] **Optimisation du seuil** : Ajuster le seuil de relevance_score (actuellement 80) via analyse ROC\n",
    "- [ ] **D√©tection anomalies simples** : Filtrer reviews dupliqu√©es, reviews g√©n√©riques (\"Great!\", \"Perfect!\")\n",
    "\n",
    "**Moyen Terme (3-6 mois)** :\n",
    "- [ ] **Fine-tuning du mod√®le BART** : Entra√Æner sur dataset Amazon labelis√© ‚Üí ‚Üë pr√©cision √† 85-90%\n",
    "- [ ] **D√©tection de spam avanc√©e** : Utiliser patterns syntaxiques + embeddings pour identifier bots\n",
    "- [ ] **Pipeline automatis√©** : D√©ployer batch quotidien sur Snowflake (Airflow/Prefect)\n",
    "- [ ] **Optimisation GPU** : D√©ployer sur AWS SageMaker ou GCP Vertex AI pour r√©duire co√ªts\n",
    "- [ ] **Analyse temporelle** : Si dates disponibles, d√©tecter trends et seasonality\n",
    "\n",
    "**Long Terme (6-12 mois)** :\n",
    "- [ ] **Mod√®le custom multi-t√¢ches** : 1 seul mod√®le pour cat√©gorie + sentiment + spam (architecture multi-head)\n",
    "- [ ] **Apprentissage des pond√©rations** : Remplacer le relevance_score statique par un mod√®le ML\n",
    "- [ ] **Clustering dynamique** : D√©couvrir automatiquement nouvelles th√©matiques √©mergentes (LDA + UMAP)\n",
    "- [ ] **A/B Testing** : Mesurer impact business sur taux de conversion, temps pass√© sur site\n",
    "- [ ] **API temps r√©el** : Exposer le mod√®le via FastAPI pour classification en temps r√©el\n",
    "- [ ] **Expansion multilingue** : Supporter chinois, japonais, allemand avec mDeBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 M√©triques de Succ√®s pour Validation\n",
    "\n",
    "Pour valider l'efficacit√© du prototype en production :\n",
    "\n",
    "| M√©trique | Objectif | Mesure |\n",
    "|----------|----------|--------|\n",
    "| **Pr√©cision cat√©gorisation** | >75% | Validation manuelle sur 500 reviews |\n",
    "| **Adoption utilisateurs** | +20% clics | Google Analytics sur reviews cat√©goris√©es |\n",
    "| **R√©duction support client** | -15% tickets | Volume tickets li√©s √† questions produit |\n",
    "| **Temps traitement** | <5s par review | Benchmark GPU vs CPU |\n",
    "| **ROI business** | +5% conversion | A/B test avec/sans relevance score |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£3Ô∏è‚É£ LIVRABLES & EXPORT {#13-livrables}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 R√©capitulatif des Livrables\n",
    "\n",
    "#### üì¶ Livrables du Case Study\n",
    "\n",
    "‚úÖ **Jupyter Notebook** : `Step_4_Case_Study_Analysis.ipynb` (ce fichier)  \n",
    "‚úÖ **SQL Queries** : `/notebooks/sql_queries/01_data_extraction.sql`  \n",
    "‚úÖ **Visualisations** : 8 graphiques dans `/data/outputs/visualizations/`  \n",
    "   - `04_eda_stats_generales.png`\n",
    "   - `04_eda_stats_par_rating.png`\n",
    "   - `07_confidence_distribution.png`\n",
    "   - `09_relevance_distribution.png`\n",
    "   - `10_category_rating_heatmap.html`\n",
    "   - `10_wordclouds_by_category.png`\n",
    "\n",
    "‚úÖ **Donn√©es pr√©par√©es** : `/data/outputs/processed/`\n",
    "   - `reviews_analyzed.parquet` (dataframe complet)\n",
    "   - `category_stats.csv` (statistiques par cat√©gorie)\n",
    "   - `top_reviews.csv` (reviews pertinentes)\n",
    "\n",
    "‚è≥ **Rapport d'analyse** : `/docs/step_4_analysis_report.md` (√† g√©n√©rer)  \n",
    "‚è≥ **Dashboard Streamlit** : `/dashboards/streamlit_app.py` (√† d√©velopper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Export du Rapport Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# G√©n√©ration automatique du rapport Markdown\n",
    "# ============================================\n",
    "# TODO: Impl√©menter la g√©n√©ration du rapport apr√®s ex√©cution compl√®te\n",
    "\n",
    "# Sections du rapport :\n",
    "# 1. Executive Summary\n",
    "# 2. Methodology\n",
    "# 3. Results & Metrics\n",
    "# 4. Business Insights\n",
    "# 5. Limitations\n",
    "# 6. Recommendations\n",
    "# 7. Appendices (SQL queries, parameters)\n",
    "\n",
    "print(\"üìÑ Template de rapport : docs/step_4_analysis_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Checklist de Compl√©tion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ Checklist Finale\n",
    "\n",
    "**√âtape 1 - Configuration** :\n",
    "- [ ] Connexion Snowflake fonctionnelle\n",
    "- [ ] Biblioth√®ques NLP install√©es (transformers, nltk)\n",
    "\n",
    "**√âtape 2 - Extraction & EDA** :\n",
    "- [ ] Extraction des donn√©es compl√®te\n",
    "- [ ] EDA avec visualisations sauvegard√©es\n",
    "- [ ] Insights cl√©s document√©s\n",
    "\n",
    "**√âtape 3 - Mod√®le NLP** :\n",
    "- [ ] Algorithme NLP s√©lectionn√© et justifi√©\n",
    "- [ ] Mod√®le zero-shot impl√©ment√©\n",
    "- [ ] Classification effectu√©e sur toutes les reviews\n",
    "\n",
    "**√âtape 4 - Validation** :\n",
    "- [ ] V√©rification de performance effectu√©e (confidence score)\n",
    "- [ ] Tests it√©ratifs document√©s (3 exp√©rimentations minimum)\n",
    "- [ ] Validation manuelle sur √©chantillon\n",
    "\n",
    "**√âtape 5 - Scoring & Visualisations** :\n",
    "- [ ] Relevance score calcul√© (5 sous-scores)\n",
    "- [ ] Visualisations finales export√©es (8 graphiques)\n",
    "- [ ] Insights business identifi√©s\n",
    "\n",
    "**√âtape 6 - Livrables** :\n",
    "- [ ] Donn√©es sauvegard√©es dans Snowflake\n",
    "- [ ] Fichiers export√©s localement (backup)\n",
    "- [ ] Dashboard Streamlit d√©velopp√© (√©tape suivante)\n",
    "- [ ] Rapport PDF g√©n√©r√© (√©tape suivante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©sum√© Ex√©cutif\n",
    "\n",
    "Ce case study a permis de :\n",
    "1. ‚úÖ D√©velopper un syst√®me de cat√©gorisation automatique (zero-shot NLP)\n",
    "2. ‚úÖ Cr√©er un relevance score multi-crit√®res (0-100)\n",
    "3. ‚úÖ Identifier [X]% de reviews \"top pertinence\" (score >80)\n",
    "4. ‚úÖ Valider la faisabilit√© technique sur [X] reviews\n",
    "\n",
    "**Prochaines √©tapes** :\n",
    "- D√©veloppement du dashboard Streamlit interactif\n",
    "- Extension √† l'ensemble des 100k+ reviews Snowflake\n",
    "- Fine-tuning du mod√®le sur dataset labellis√©\n",
    "- D√©ploiement en production (pipeline automatis√©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö R√âF√âRENCES\n",
    "\n",
    "- [Hugging Face - Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification)\n",
    "- [VADER Sentiment Analysis](https://github.com/cjhutto/vaderSentiment)\n",
    "- [Snowflake Python Connector Docs](https://docs.snowflake.com/en/user-guide/python-connector)\n",
    "- [Streamlit Documentation](https://docs.streamlit.io)\n",
    "- [BART Model Paper](https://arxiv.org/abs/1910.13461)\n",
    "- [mDeBERTa Model](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
